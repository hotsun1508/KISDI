{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hotsun1508/KISDI/blob/main/LinkedIn_Scraper_for_50_Profiles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import time\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import re\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "COOKIES_FILE = \"cookies.txt\"\n",
        "# Set to a very high number to capture posts from the last ~2.5 years.\n",
        "# The script will stop automatically if it reaches the end of the profile's feed.\n",
        "MAX_POST_SCROLLS = 250\n",
        "SCROLL_PAUSE_TIME = 2.5 # Increase if your internet connection is slow\n",
        "\n",
        "profile_urls = [\n",
        "    \"https://www.linkedin.com/in/narendramodi/\", \"https://www.linkedin.com/in/williamhgates/\",\n",
        "    \"https://www.linkedin.com/in/hamdan-bin-mohammed-al-maktoum-72761524a/\", \"https://www.linkedin.com/in/barackobama/\",\n",
        "    \"https://www.linkedin.com/in/satyanadella/\", \"https://www.linkedin.com/in/sundarpichai/\",\n",
        "    \"https://www.linkedin.com/in/mark-carney-5b9744205/\", \"https://www.linkedin.com/in/nithin-kamath-81136242/\",\n",
        "    \"https://www.linkedin.com/in/andrewyng/\", \"https://www.linkedin.com/in/adammgrant/\",\n",
        "    \"https://www.linkedin.com/in/ursula-von-der-leyen/\", \"https://www.linkedin.com/in/antonio-guterres/\",\n",
        "    \"https://www.linkedin.com/in/hh-sheikh-mohamed-bin-zayed-al-nahyan/\", \"https://www.linkedin.com/in/giorgiameloni/\",\n",
        "    \"https://www.linkedin.com/in/rafanadal/\", \"https://www.linkedin.com/in/alexxubyte/\",\n",
        "    \"https://www.linkedin.com/in/aravind-srinivas-16051987/\", \"https://www.linkedin.com/in/piyushgoyalofficial/\",\n",
        "    \"https://www.linkedin.com/in/shantanu-naidu/\", \"https://www.linkedin.com/in/mario-sergio-cortella/\",\n",
        "    \"https://www.linkedin.com/in/nikhilkamathcio/\", \"https://www.linkedin.com/in/midudev/\",\n",
        "    \"https://www.linkedin.com/in/andrew-huberman/\", \"https://www.linkedin.com/in/sel%C3%A7uk-bayraktar-a54bb619/\",\n",
        "    \"https://www.linkedin.com/in/lawrence-wong-15728a18/\", \"https://www.linkedin.com/in/jean-marc-jancovici/\",\n",
        "    \"https://www.linkedin.com/in/pauloguedeseconomista/\", \"https://www.linkedin.com/in/emmanuelmacron/\",\n",
        "    \"https://www.linkedin.com/in/ekremimamoglu/\", \"https://www.linkedin.com/in/mattgarman/\",\n",
        "    \"https://www.linkedin.com/in/sarablakely27/\", \"https://www.linkedin.com/in/emollick/\",\n",
        "    \"https://www.linkedin.com/in/zak-brown-46b168104/\", \"https://www.linkedin.com/in/revant-himatsingka-food-pharmer-68326126/\",\n",
        "    \"https://www.linkedin.com/in/aadit-palicha/\", \"https://www.linkedin.com/in/guillaume-pley-a1877731/\",\n",
        "    \"https://www.linkedin.com/in/lexfridman/\", \"https://www.linkedin.com/in/raydalio/\",\n",
        "    \"https://www.linkedin.com/in/antoniofilosa/\", \"https://www.linkedin.com/in/tonyelumelu/\",\n",
        "    \"https://www.linkedin.com/in/bodour-al-qasimi-61a79b165/\", \"https://www.linkedin.com/in/mr-beast/\",\n",
        "    \"https://www.linkedin.com/in/danielpink/\", \"https://www.linkedin.com/in/andersonrcorreia/\",\n",
        "    \"https://www.linkedin.com/in/buschroland/\", \"https://www.linkedin.com/in/larrymadowo/\",\n",
        "    \"https://www.linkedin.com/in/sirlewishamilton/\", \"https://www.linkedin.com/in/alicjasmin/\",\n",
        "    \"https://www.linkedin.com/in/dharmesh/\", \"https://www.linkedin.com/in/aman-gupta-7217a515/\"\n",
        "]\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "# --- HELPER FUNCTIONS ---\n",
        "def load_cookies(browser, file_path):\n",
        "    \"\"\"Loads cookies from a Netscape-formatted file into the browser session.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            if not line.startswith('#') and line.strip():\n",
        "                fields = line.strip().split('\\t')\n",
        "                if len(fields) == 7:\n",
        "                    browser.add_cookie({\n",
        "                        'name': fields[5], 'value': fields[6], 'domain': fields[0],\n",
        "                        'path': fields[2], 'expiry': int(fields[4]) if fields[4].isdigit() else None,\n",
        "                        'secure': fields[3] == 'TRUE'\n",
        "                    })\n",
        "\n",
        "def convert_abbreviated_to_number(s):\n",
        "    \"\"\"Converts strings like '1K' or '2.5M' to integers.\"\"\"\n",
        "    if not isinstance(s, str): return 0\n",
        "    s = s.upper().strip().replace(',', '')\n",
        "    if 'K' in s:\n",
        "        return int(float(s.replace('K', '')) * 1000)\n",
        "    elif 'M' in s:\n",
        "        return int(float(s.replace('M', '')) * 1_000_000)\n",
        "    else:\n",
        "        try:\n",
        "            return int(s)\n",
        "        except ValueError:\n",
        "            return 0\n",
        "\n",
        "# --- COMMENT SCRAPING FUNCTIONS ---\n",
        "def expand_all_comments(browser):\n",
        "    \"\"\"Continuously clicks 'Show previous comments' and 'Load previous replies'.\"\"\"\n",
        "    while True:\n",
        "        buttons_found = False\n",
        "        try:\n",
        "            show_more_buttons = browser.find_elements(By.XPATH, \"//button[span[text()='Show previous comments']]\")\n",
        "            for button in show_more_buttons:\n",
        "                browser.execute_script(\"arguments[0].click();\", button)\n",
        "                buttons_found = True\n",
        "                time.sleep(1.5)\n",
        "\n",
        "            load_replies_buttons = browser.find_elements(By.XPATH, \"//button[span[text()='Load previous replies']]\")\n",
        "            for button in load_replies_buttons:\n",
        "                browser.execute_script(\"arguments[0].click();\", button)\n",
        "                buttons_found = True\n",
        "                time.sleep(1.5)\n",
        "\n",
        "            if not buttons_found:\n",
        "                print(\"[*] No more 'show/load' buttons found.\")\n",
        "                break\n",
        "        except (NoSuchElementException, StaleElementReferenceException):\n",
        "            print(\"[*] All comment expansion buttons handled.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"[!] Error during comment expansion: {e}\")\n",
        "            break\n",
        "\n",
        "def parse_and_save_comments(html, post_id, comments_csv_path):\n",
        "    \"\"\"Parses the HTML of a post page to extract all comments and replies.\"\"\"\n",
        "    soup = bs(html, 'html.parser')\n",
        "    # **FIX**: Updated selector to find the main container for each comment.\n",
        "    comment_wrappers = soup.find_all(\"article\", class_=lambda x: x and \"comments-comment-entity\" in x)\n",
        "\n",
        "    comments_data = []\n",
        "    for comment_wrapper in comment_wrappers:\n",
        "        try:\n",
        "            # **FIX**: Updated selector for the commenter's name.\n",
        "            commenter_name_tag = comment_wrapper.find(\"span\", class_=\"comments-comment-meta__description-title\")\n",
        "            commenter_name = commenter_name_tag.get_text(strip=True) if commenter_name_tag else \"N/A\"\n",
        "\n",
        "            # **FIX**: Updated selector for the main comment text content.\n",
        "            comment_text_div = comment_wrapper.find(\"div\", class_=\"update-components-text\")\n",
        "            comment_text = comment_text_div.get_text(separator=\"\\n\", strip=True) if comment_text_div else \"\"\n",
        "\n",
        "            # **FIX**: Updated selector for finding the likes count button.\n",
        "            likes_button = comment_wrapper.find(\"button\", class_=lambda x: x and \"reactions-count\" in x)\n",
        "            if likes_button:\n",
        "                likes_span = likes_button.find(\"span\", class_=\"v-align-middle\")\n",
        "                comment_likes = convert_abbreviated_to_number(likes_span.get_text(strip=True)) if likes_span else 0\n",
        "            else:\n",
        "                comment_likes = 0\n",
        "\n",
        "            # **FIX**: Updated logic to determine if a comment is a reply.\n",
        "            is_reply = \"comments-comment-entity--reply\" in comment_wrapper.get('class', [])\n",
        "\n",
        "            comments_data.append({\n",
        "                \"Post_ID\": post_id, \"Commenter_Name\": commenter_name,\n",
        "                \"Comment_Text\": comment_text, \"Comment_Likes\": comment_likes,\n",
        "                \"Is_Reply\": is_reply\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"[!] Error parsing a single comment: {e}\")\n",
        "            continue\n",
        "\n",
        "    with open(comments_csv_path, mode='a', encoding='utf-8', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=[\"Post_ID\", \"Commenter_Name\", \"Comment_Text\", \"Comment_Likes\", \"Is_Reply\"])\n",
        "        writer.writerows(comments_data)\n",
        "\n",
        "    return len(comments_data)\n",
        "\n",
        "# --- MAIN SCRIPT LOGIC ---\n",
        "def main():\n",
        "    \"\"\"Main function to orchestrate the scraping process.\"\"\"\n",
        "    chrome_options = Options()\n",
        "    # chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "    print(\"[*] Initializing Chrome driver...\")\n",
        "    browser = webdriver.Chrome(options=chrome_options)\n",
        "    browser.set_window_size(1920, 1080)\n",
        "\n",
        "    print(f\"[*] Loading cookies from {COOKIES_FILE}...\")\n",
        "    browser.get('https://www.linkedin.com/')\n",
        "    time.sleep(2)\n",
        "    load_cookies(browser, COOKIES_FILE)\n",
        "    browser.refresh()\n",
        "\n",
        "    try:\n",
        "        WebDriverWait(browser, 20).until(EC.presence_of_element_located((By.ID, \"global-nav\")))\n",
        "        print(\"[*] Login successful.\")\n",
        "    except TimeoutException:\n",
        "        print(\"[!] Login failed. Check your cookies.txt file. Exiting.\")\n",
        "        browser.quit()\n",
        "        return\n",
        "\n",
        "    for profile_url in profile_urls:\n",
        "        profile_name = profile_url.strip('/').split('/')[-1]\n",
        "        posts_csv_path = f\"{profile_name}_posts.csv\"\n",
        "        comments_csv_path = f\"{profile_name}_comments.csv\"\n",
        "\n",
        "        print(f\"\\n--- Scraping profile: {profile_name} ---\")\n",
        "\n",
        "        with open(posts_csv_path, mode='w', encoding='utf-8', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"Post_ID\", \"Post_URL\", \"Author\", \"Post_Time\", \"Content\", \"Reactions\", \"Comment_Count\"])\n",
        "        with open(comments_csv_path, mode='w', encoding='utf-8', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"Post_ID\", \"Commenter_Name\", \"Comment_Text\", \"Comment_Likes\", \"Is_Reply\"])\n",
        "\n",
        "        try:\n",
        "            activity_url = f\"{profile_url.strip('/')}/recent-activity/all/\"\n",
        "            browser.get(activity_url)\n",
        "            time.sleep(5)\n",
        "\n",
        "            print(f\"[*] Scrolling profile feed for up to {MAX_POST_SCROLLS} iterations...\")\n",
        "            last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
        "            for i in range(MAX_POST_SCROLLS):\n",
        "                print(f\"  > Scroll {i+1}/{MAX_POST_SCROLLS}\")\n",
        "                browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "                time.sleep(SCROLL_PAUSE_TIME)\n",
        "                new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
        "                if new_height == last_height:\n",
        "                    print(\"[*] Reached the end of the page.\")\n",
        "                    break\n",
        "                last_height = new_height\n",
        "\n",
        "            print(\"[*] Parsing post feed...\")\n",
        "            soup = bs(browser.page_source, \"html.parser\")\n",
        "            post_wrappers = soup.find_all(\"div\", class_=lambda x: x and \"feed-shared-update-v2\" in x)\n",
        "\n",
        "            posts_to_visit = []\n",
        "            unique_post_ids = set()\n",
        "            for pw in post_wrappers:\n",
        "                data_urn = pw.get(\"data-urn\", \"\")\n",
        "                if \"urn:li:activity:\" not in data_urn: continue\n",
        "\n",
        "                post_id = data_urn.split(\":\")[-1]\n",
        "                if post_id in unique_post_ids: continue\n",
        "                unique_post_ids.add(post_id)\n",
        "\n",
        "                post_url = f\"https://www.linkedin.com/feed/update/{data_urn}/\"\n",
        "                author_name = pw.find(\"span\", {\"class\": \"update-components-actor__name\"}).get_text(strip=True) if pw.find(\"span\", {\"class\": \"update-components-actor__name\"}) else \"N/A\"\n",
        "                post_time = pw.find(\"span\", {\"class\": \"update-components-actor__sub-description\"}).get_text(strip=True) if pw.find(\"span\", {\"class\": \"update-components-actor__sub-description\"}) else \"N/A\"\n",
        "                content_div = pw.find(\"div\", {\"class\": \"update-components-text\"})\n",
        "                post_content = content_div.get_text(separator=\"\\n\", strip=True) if content_div else \"\"\n",
        "                reactions_span = pw.find(\"span\", {\"class\": \"social-details-social-counts__reactions-count\"})\n",
        "                post_reactions = convert_abbreviated_to_number(reactions_span.get_text(strip=True)) if reactions_span else 0\n",
        "                comments_li = pw.find(\"li\", {\"class\": \"social-details-social-counts__comments\"})\n",
        "                comment_count = convert_abbreviated_to_number(re.match(r\"[\\d,KkMm]+\", comments_li.get_text(strip=True)).group(0)) if comments_li else 0\n",
        "\n",
        "                posts_to_visit.append({\"id\": post_id, \"url\": post_url})\n",
        "\n",
        "                with open(posts_csv_path, mode='a', encoding='utf-8', newline='') as file:\n",
        "                    writer = csv.writer(file)\n",
        "                    writer.writerow([post_id, post_url, author_name, post_time, post_content, post_reactions, comment_count])\n",
        "\n",
        "            print(f\"[*] Found {len(posts_to_visit)} unique posts. Now visiting each to scrape comments.\")\n",
        "\n",
        "            for i, post in enumerate(posts_to_visit):\n",
        "                try:\n",
        "                    print(f\"\\n--- Visiting post {i+1}/{len(posts_to_visit)} (ID: {post['id']}) ---\")\n",
        "                    browser.get(post['url'])\n",
        "                    time.sleep(5)\n",
        "\n",
        "                    print(\"[*] Expanding all comments...\")\n",
        "                    expand_all_comments(browser)\n",
        "\n",
        "                    print(\"[*] Parsing and saving comments...\")\n",
        "                    comment_count = parse_and_save_comments(browser.page_source, post['id'], comments_csv_path)\n",
        "                    print(f\"[*] Saved {comment_count} comments for post {post['id']}.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ FAILED to scrape post {post['id']}. Error: {e}. Skipping to next post.\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ A critical error occurred for profile {profile_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    browser.quit()\n",
        "    print(\"\\n[*] All profiles processed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "_sD8QVkV1S48"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}